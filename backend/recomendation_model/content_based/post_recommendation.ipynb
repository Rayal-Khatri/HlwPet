{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>Community ID</th>\n",
       "      <th>Community Name</th>\n",
       "      <th>Author ID</th>\n",
       "      <th>Author Username</th>\n",
       "      <th>Created At</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>62</td>\n",
       "      <td>hello-pet</td>\n",
       "      <td>1</td>\n",
       "      <td>hansome</td>\n",
       "      <td>2024-02-16 15:24:34.994040+00:00</td>\n",
       "      <td>hello</td>\n",
       "      <td>good morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>62</td>\n",
       "      <td>hello-pet</td>\n",
       "      <td>448</td>\n",
       "      <td>aaron94</td>\n",
       "      <td>2024-02-29 17:01:47.086217+00:00</td>\n",
       "      <td>The dog parks are doubling</td>\n",
       "      <td>My city tried out a new off-leash dog park for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>63</td>\n",
       "      <td>dogs lover</td>\n",
       "      <td>86</td>\n",
       "      <td>aaronphillips</td>\n",
       "      <td>2024-02-29 17:03:01.017136+00:00</td>\n",
       "      <td>Held prisoner by dogs, waiting to get attacked.</td>\n",
       "      <td>My neighbors don't keep their dogs contained. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>63</td>\n",
       "      <td>dogs lover</td>\n",
       "      <td>317</td>\n",
       "      <td>abbottjulie</td>\n",
       "      <td>2024-02-29 17:03:21.637885+00:00</td>\n",
       "      <td>Dogs in ABC's</td>\n",
       "      <td>I'm sick of this crazy dog culture! I'm lookin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>64</td>\n",
       "      <td>petmandu</td>\n",
       "      <td>137</td>\n",
       "      <td>abigailhill</td>\n",
       "      <td>2024-02-29 17:03:47.620809+00:00</td>\n",
       "      <td>Tried dating a guy with a dog (cautionary tale)</td>\n",
       "      <td>Alright so. Ill spare you the deets and give y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>86</td>\n",
       "      <td>66</td>\n",
       "      <td>smart pet</td>\n",
       "      <td>179</td>\n",
       "      <td>williamsjennifer</td>\n",
       "      <td>2024-02-29 17:28:30.569599+00:00</td>\n",
       "      <td>Pet safe rug cleaning solution?</td>\n",
       "      <td>Got myself a nice rug cleaner recently. As we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>87</td>\n",
       "      <td>62</td>\n",
       "      <td>hello-pet</td>\n",
       "      <td>390</td>\n",
       "      <td>jennifer62</td>\n",
       "      <td>2024-02-29 17:28:49.710914+00:00</td>\n",
       "      <td>Are there any ethically ‚Äúcaged pets‚Äù</td>\n",
       "      <td>I adore animals, my current landlord only allo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>88</td>\n",
       "      <td>63</td>\n",
       "      <td>dogs lover</td>\n",
       "      <td>329</td>\n",
       "      <td>cheryldaniel</td>\n",
       "      <td>2024-02-29 17:30:49.925274+00:00</td>\n",
       "      <td>Why are dog owners so sensitive?</td>\n",
       "      <td>Guy I was talking to kept referring to his dog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>89</td>\n",
       "      <td>63</td>\n",
       "      <td>dogs lover</td>\n",
       "      <td>244</td>\n",
       "      <td>tara46</td>\n",
       "      <td>2024-02-29 17:31:18.785730+00:00</td>\n",
       "      <td>A dog's life is more precious than a human life</td>\n",
       "      <td>I went to my auto insurance office to renegoti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>90</td>\n",
       "      <td>63</td>\n",
       "      <td>dogs lover</td>\n",
       "      <td>166</td>\n",
       "      <td>lisa60</td>\n",
       "      <td>2024-02-29 17:31:54.928259+00:00</td>\n",
       "      <td>Tried dating a guy with a dog (cautionary tale)</td>\n",
       "      <td>Alright so. Ill spare you the deets and give y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Post ID  Community ID Community Name  Author ID   Author Username  \\\n",
       "0        15            62      hello-pet          1           hansome   \n",
       "1        16            62      hello-pet        448           aaron94   \n",
       "2        17            63     dogs lover         86     aaronphillips   \n",
       "3        18            63     dogs lover        317       abbottjulie   \n",
       "4        19            64       petmandu        137       abigailhill   \n",
       "..      ...           ...            ...        ...               ...   \n",
       "71       86            66      smart pet        179  williamsjennifer   \n",
       "72       87            62      hello-pet        390        jennifer62   \n",
       "73       88            63     dogs lover        329      cheryldaniel   \n",
       "74       89            63     dogs lover        244            tara46   \n",
       "75       90            63     dogs lover        166            lisa60   \n",
       "\n",
       "                          Created At  \\\n",
       "0   2024-02-16 15:24:34.994040+00:00   \n",
       "1   2024-02-29 17:01:47.086217+00:00   \n",
       "2   2024-02-29 17:03:01.017136+00:00   \n",
       "3   2024-02-29 17:03:21.637885+00:00   \n",
       "4   2024-02-29 17:03:47.620809+00:00   \n",
       "..                               ...   \n",
       "71  2024-02-29 17:28:30.569599+00:00   \n",
       "72  2024-02-29 17:28:49.710914+00:00   \n",
       "73  2024-02-29 17:30:49.925274+00:00   \n",
       "74  2024-02-29 17:31:18.785730+00:00   \n",
       "75  2024-02-29 17:31:54.928259+00:00   \n",
       "\n",
       "                                              Title  \\\n",
       "0                                             hello   \n",
       "1                        The dog parks are doubling   \n",
       "2   Held prisoner by dogs, waiting to get attacked.   \n",
       "3                                     Dogs in ABC's   \n",
       "4   Tried dating a guy with a dog (cautionary tale)   \n",
       "..                                              ...   \n",
       "71                  Pet safe rug cleaning solution?   \n",
       "72             Are there any ethically ‚Äúcaged pets‚Äù   \n",
       "73                 Why are dog owners so sensitive?   \n",
       "74  A dog's life is more precious than a human life   \n",
       "75  Tried dating a guy with a dog (cautionary tale)   \n",
       "\n",
       "                                              Content  \n",
       "0                                        good morning  \n",
       "1   My city tried out a new off-leash dog park for...  \n",
       "2   My neighbors don't keep their dogs contained. ...  \n",
       "3   I'm sick of this crazy dog culture! I'm lookin...  \n",
       "4   Alright so. Ill spare you the deets and give y...  \n",
       "..                                                ...  \n",
       "71  Got myself a nice rug cleaner recently. As we ...  \n",
       "72  I adore animals, my current landlord only allo...  \n",
       "73  Guy I was talking to kept referring to his dog...  \n",
       "74  I went to my auto insurance office to renegoti...  \n",
       "75  Alright so. Ill spare you the deets and give y...  \n",
       "\n",
       "[76 rows x 8 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('post_dataset.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating a tags of all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Content\"] = df[\"Content\"].apply(lambda x:x.split())\n",
    "df[\"Title\"] = df[\"Title\"].apply(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tags\"] =df[\"Content\"] + df[\"Title\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tags\"] = df[\"tags\"].apply(lambda x: \" \".join(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## changing to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def to_lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tags\"]=to_lower(df[\"tags\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove html text patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "@np.vectorize\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile(\"<.*?>\")\n",
    "    return pattern.sub(r\"\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tags\"] = remove_html_tags(df[\"tags\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "exclude = string.punctuation\n",
    "\n",
    "print(exclude)\n",
    "@np.vectorize\n",
    "def remove_punc(text):\n",
    "    clean_text = str.maketrans(\"\", \"\", exclude)\n",
    "   \n",
    "    return text.translate(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('ask ram and hari for help', dtype='<U25')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punc(\"ask @ram and @hari for help\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tags\"] = remove_punc(df[\"tags\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handeling slang words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations_dict = {}\n",
    "\n",
    "with open(\"./slang.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        abbreviation, full_form = map(str.strip, line.lower().split(\"=\"))\n",
    "\n",
    "        abbreviations_dict[abbreviation] = full_form\n",
    "\n",
    "@np.vectorize\n",
    "def handle_slang(text):\n",
    "    for abbreviation, full_form in abbreviations_dict.items():\n",
    "        text = text.replace(abbreviation, full_form)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('it was mistake as far as i know', dtype='<U31')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_slang(\"it was mistake afaik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tags\"] = handle_slang(df[\"tags\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handeling emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "@np.vectorize\n",
    "def handle_emoji(text):\n",
    "    text_with_emojis = emoji.demojize(text)\n",
    "    return text_with_emojis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('hi there :fire:', dtype='<U15')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_emoji(\"hi there üî•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tags\"] = handle_emoji(df[\"tags\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correct spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def correct_words(text):\n",
    "    txtblb = TextBlob(text)\n",
    "    corrected_text = \" \".join(txtblb.correct().words)\n",
    "    return corrected_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('it is the best selling book', dtype='<U27')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"it is the best splling book\"\n",
    "\n",
    "correct_words(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tags\"] = correct_words(df[\"tags\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemming the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "@np.vectorize\n",
    "def stemmer(text):\n",
    "    return ps.stem(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array('run', dtype='<U3'),\n",
       " array('tree', dtype='<U4'),\n",
       " array('run', dtype='<U3'))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer(\"running\"),stemmer(\"trees\"),stemmer(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tags\"] = stemmer(df[\"tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lematize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def lemmatize_sentence(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_words = word_tokenize(sentence)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_words]\n",
    "    lemmatized_sentence = \" \".join(lemmatized_words)\n",
    "    return lemmatized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('mouse are running', dtype='<U17')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_sentence(\"mice are running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tags\"] = lemmatize_sentence(df[\"tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove stops words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total stop words: 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ripple\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "total_stop_words,stop_words= len(list(stopwords.words(\"english\"))), list(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "print(f\"total stop words: {total_stop_words}\")\n",
    "stop_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorize using BOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=5000, stop_words=\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = cv.fit_transform(df[\"tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [2, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64),\n",
       " (76, 2201))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.toarray(),vector.toarray().shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exporting processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"preprocesses_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exoprting vector and vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vector, open(\"utils/vector.pkl\", \"wb\"))\n",
    "pickle.dump(cv, open(\"utils/cv.pkl\", \"wb\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
